# Cloud-Based ETL Data Pipeline using AWS Services ![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)

## ğŸ“˜ Project Overview
This project demonstrates a cloud-based ETL (Extractâ€“Transformâ€“Load) data pipeline built using Apache Airflow on GCP and AWS services (S3, Glue, Data Catalog).
It automates the extraction of NYC Green Taxi Trip data, transforms and stores it as Parquet files, and registers datasets for analytics and future transformations.

---
## ğŸ” System Architecture

![RAG Workflow](assets/images/architecture.png)
- 1ï¸âƒ£ The developer writes or updates Airflow DAGs and ETL scripts locally.
Once the code is ready, itâ€™s pushed to a GitHub repository. This repo serves as the single source of truth for all DAGs and pipeline scripts.
- 2ï¸âƒ£ A GCP Virtual Machine hosts Apache Airflow, which is configured to automatically pull the latest DAGs from the GitHub repository.
Whenever a developer pushes code, the DAGs folder on the GCP VM is synced, ensuring Airflow always runs the most up-to-date workflow.
- 3ï¸âƒ£ Airflow orchestrates the ETL job. The DAG extracts NYC Green Taxi Trip data (from the public dataset URL), processes it into Parquet format, and uploads the latest 12 months of data to an AWS S3 bucket.
- 4ï¸âƒ£ Once new data lands in S3, an AWS Glue Crawler is triggered.
The crawler scans the S3 bucket, infers the schema, and registers metadata in the AWS Glue Data Catalog, making the data discoverable and queryable across AWS services
- 5ï¸âƒ£ Using the cataloged data, AWS Glue ETL Jobs can now perform transformations, cleansing, or enrichment tasks.
- 6ï¸âƒ£ The processed data from Glue is (or will be) loaded into Amazon Redshift, enabling SQL-based analytics and integration with BI tools such as QuickSight, Tableau, or Looker
---

## ğŸ’¡ Example: Streamlit App in Action

Once you run the Streamlit app, youâ€™ll see an interface like this:
![RAG PDF Q&A Example](assets/app_example.png)

Ask questions about your PDF (for example):

## Requirements

- Python 3.10+
- Libraries (can be installed via `requirements.txt`):

```bash
pip install -r requirements.txt
```
---

## Project Structure
```
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ vector_store/        # Vector DB storage (ignored by git)
â”œâ”€â”€ models/
â”‚   â””â”€â”€ Dolphin3.0-Llama3.2-1B-Q4_K_M.gguf   # Local LLaMA model (ignored by git)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest_pdf.py        # Load PDF and extract text
â”‚   â”œâ”€â”€ split_text.py        # Split text into chunks
â”‚   â”œâ”€â”€ create_vectorstore.py # Create embeddings & Chroma vector store
â”‚   â”œâ”€â”€ RAG_pipeline.py      # Main functions: ask_question, load_llama_model
â”‚   â””â”€â”€ app.py               # Streamlit app
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```
---

## Project Setup
### 1. Clone the repository

```bash
git clone https://github.com/Hashem-Qaryouti/llama3-rag-pipeline.git
cd <your-repo-name>
```
### 2. Create and activate a virtual environment
```bash
python -m venv llama_env
```
#### Activate the environment:
- **Windows:**
```bash
llama_env\Scripts\activate
```
- **Linux:**
```bash
source llama_env/bin/activate
```
### 3. Install dependencies
```bash
pip install -r requirements.txt

```

### 4. Add your PDF
Place your PDF file in the `data/` folder.
Update paths.py or the PDF_PATH variable in `ingest_pdf.py` to point to your PDF file.

### 5. Add your LLaMA model

Place your downloaded LLaMA GGUF model in the `models/` folder.
Example:
```bash
models/Dolphin3.0-Llama3.2-1B-Q4_K_M.gguf
```
---

## Usage
1. Run the Streamlit App
```bash
cd src
streamlit run app.py
```
- **Open the URL shown in your terminal (usually http://localhost:8501):**
- **Enter your question in the text box and click Ask**
- **View the answer generated by LLaMA based on your PDF content**
